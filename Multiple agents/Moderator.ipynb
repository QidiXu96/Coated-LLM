{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderator(client, question, response, feedback, model_name=\"claude-3-opus-20240229\"):\n",
    "    try:\n",
    "        # Create the prompt with the provided question, response, and feedback\n",
    "        prompt = \"Previous response: \" + response + \"\\n\\n\" + \"Feedback: \" + feedback + \"\\n\\n\" + \"Based on the previous response and feedback, \" + question + \"Take a breath and work on this problem step by step. And conclude using the format 'Effective in theory: <Positive or Non-positive>.'\"\n",
    "\n",
    "        # Call the model to generate the final reasoning\n",
    "        message = client.messages.create(\n",
    "            model=model_name,\n",
    "            max_tokens=1500,\n",
    "            temperature=0.7,\n",
    "            system=\"You are an expert in therapy development for Alzheimer's disease and you are trying to decide if the combination of two drugs is effective or not to treat or slow the progression of Alzheimer's disease in theory. As a proficient neurobiologist, use your own knowledge and search for external information if necessary. Also, it is rare that combination of two drugs become efficacious and synergistic in the real world.\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Extract the final reasoning from the model's response\n",
    "        final_reasoning = message.content[0].text\n",
    "        return final_reasoning\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_multiple_runs(reasoning_file, feedback_file, client, batch_size=10, runs=5, results_path='.'):\n",
    "    # Load chains of thoughts and feedback from the provided files\n",
    "    with open(reasoning_file, 'r') as file:\n",
    "        chains = json.load(file)\n",
    "    with open(feedback_file, 'r') as file:\n",
    "        fb = json.load(file)\n",
    "        \n",
    "    keys = list(chains.keys())\n",
    "    total = len(keys)\n",
    "\n",
    "    # Ensure the results path exists\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    for run in range(1, runs + 1):\n",
    "        final_reasoning = {}\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_keys = keys[i:i + batch_size]\n",
    "            for key in batch_keys:\n",
    "                question = chains[key]['question']\n",
    "                response = chains[key]['chain_of_thoughts']\n",
    "                feedback = fb[key]\n",
    "                final_reasoning[key] = moderator(client, question, response, feedback)\n",
    "                time.sleep(2)  # Sleep to prevent hitting rate limits\n",
    "        \n",
    "        # Save the results of the current run to the specified path\n",
    "        result_file = os.path.join(results_path, f'final_answer_with_feedback_{run}.json')\n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(final_reasoning, f, indent=4)\n",
    "        \n",
    "        print(f\"Run {run} completed and saved to {result_file}.\")\n",
    "\n",
    "    print(\"All runs processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
